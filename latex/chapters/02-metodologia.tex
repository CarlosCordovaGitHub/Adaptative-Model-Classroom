\chapter{METODOLOGÍA}

\section{Enfoque y diseño de la investigación}

En el desarrollo del Sistema de Evaluación Adaptativa se utilizó un enfoque de investigación cuantitativa en el que la evaluación era objetiva, numérica, reproducible y medible de las variables pedagógicas y computacionales. Este enfoque era lógico ya que se relaciona con el tipo de problema abordado, es decir, optimizar procesos de evaluación y validar un sistema de software fundamentado en modelos matemáticos, estadísticos y probabilísticos que centra la evaluación en niveles profundos del conocimiento del estudiante.\\

El método cuantitativo permite la evaluación de la forma de actuar del motor adaptativo mediante la valoración de indicadores con los que se puede medir, como pueden ser:

\begin{itemize}
    \item La estimación de la habilidad latente del aprendiz ($\theta$),
    \item El ajuste en base a la precisión de las métricas como el error cuadrático medio (RMSE),
    \item La fiabilidad de las probabilidades analizada con la métrica de Brier Score;
\end{itemize}

además, tomando métricas concretas de la ingeniería de ciencias computacionales como:
    
\begin{itemize}
    \item La latencia de la respuesta del sistema,
    \item Percentiles de tiempo de procesamiento (P50 y P95),
    \item Y la tasa de peticiones por segundo (RPS).
\end{itemize}

Estas métricas sirven para el diagnóstico en base a criterios cuantificables de la precisión, la eficiencia y la escalabilidad del sistema propuesto.\\

La utilización de esta vertiente metodológica se apoya en la documentación especializada de los sistemas de aprendizaje adaptativo y la evaluación psicométrica, la cual establece que para la obtención de indicadores robustos de aprendizaje deben emplearse modelos estadísticos que permiten inferir variables latentes a partir de la evidencia empírica observable, particularmente en el caso de la Teoría de Respuesta al Ítem (IRT) y en los modelos bayesianos de rastreo de conocimiento (BKT) \cite{ref4,ref5,ref12}.\\

\begin{enumerate}
\item \textbf{Clasificación de la investigación}
\vspace{0.5cm}

Tomando en cuenta el marco metodológico que se ha seguido, esta investigación puede ser clasificada como una investigación tecnológica aplicada, la cual se halla orientada al diseño, a la validación y a la implementación de un artefacto de software funcional y operativo que tiene la finalidad de proporcionar una solución a un problema de práctica educativa en contextos locales, específicamente, la modulación adaptativa de evaluaciones mediante la integración de modelos de Machine Learning en la educación superior  \cite{ref7,ref10}.\\

\end{enumerate}

\subsection{Arquitectura experimental y estrategia de simulación}
\vspace{0.5cm}

La arquitectura experimental se apoya en la simulación computacional, pues las limitaciones logísticas, éticas y operativas de llevar a cabo un elevado número de pruebas con estudiantes reales en una fase incipiente del desarrollo nos llevaron a adoptar esta opción metodológica. En este contexto, la simulación estocástica y los métodos Monte Carlo conforman una opción metodológica argumentada teóricamente, pero también muy extendida y validada en la literatura para evaluar sistemas adaptativos complejos  \cite{ref3,ref9,ref10}.\\

Este modelo propone la construcción de un entorno de simulación en el que fueron modelados perfiles de estudiantes virtuales con ciertos parámetros psicométricos controlados, entre los que podemos encontrar el nivel de habilidad inicial  ($\theta$), la consistencia de respuesta ante ítems de dificultad variable y el ratio de aprendizaje. Este entorno permite generar un elevado número de interacciones simuladas entre el sistema y perfiles de estudiantes heterogéneos, lo que permite estudiar la convergencia del algoritmo adaptativo, su comportamiento bajo distintas condiciones operativas y evaluar la robustez frente a situaciones adversas o excepcionales.\\

De igual manera, la simulación computacional favoreció la validación operativa del sistema en situaciones de baja probabilidad de ocurrencia o difícilmente reproducibles en contextos reales de aplicación, tales como patrones de respuestas erráticas por parte de los estudiantes, ejecución de múltiples sesiones de evaluación de manera concurrente, y escenarios de escasez de ítems calibrados en el banco de preguntas. Todo ello contribuyó a proporcionar consistencia empírica a la evaluación de la tolerancia a fallos y la robustez estructural del motor adaptativo.\\

En el siguiente apartado se presenta la relación detallada de las variables e indicadores de validación en la Tabla~\ref{tab:criterios-cuantitativos}, la cual recoge la síntesis estructurada de los criterios cuantitativos que se han establecido para la evaluación sistemática del desempeño del motor adaptativo.

\begin{table}[H]
\centering
\caption{Criterios cuantitativos utilizados para la validación del motor adaptativo}
\label{tab:criterios-cuantitativos}
\renewcommand{\arraystretch}{1.2}
\begin{spacing}{1}
\begin{tabular}{|p{4.0cm}|p{5.5cm}|p{5.5cm}|}
\hline
\textbf{Variable operacionalizada} & \textbf{Descripción conceptual} & \textbf{Función en el proceso de validación} \\
\hline
$\theta$ (parámetro de habilidad estimado) &
Inferencia del nivel latente de dominio cognitivo del estudiante &
Evaluar la precisión diagnóstica del modelo psicométrico \\
\hline
RMSE / MAE &
Cuantificación del error entre el parámetro real de habilidad y su estimación computacional &
Medir exactitud predictiva del sistema implementado \\
\hline
Brier Score &
Función de pérdida cuadrática aplicada a probabilidades predichas &
Evaluar calidad y calibración de las predicciones probabilísticas \\
\hline
Latencia (ms) &
Duración temporal del procesamiento de peticiones del sistema &
Analizar rendimiento computacional y eficiencia algorítmica \\
\hline
RPS &
Volumen de peticiones procesadas exitosamente por unidad temporal &
Evaluar escalabilidad horizontal y capacidad de concurrencia \\
\hline
P50 / P95 &
Percentiles de la distribución de tiempos de respuesta &
Detectar degradación del rendimiento bajo condiciones de carga elevada \\
\hline
\end{tabular}
\end{spacing}
\end{table}


\section{Tipo y diseño de la investigación}

La investigación desarrollada se enmarca dentro del ámbito de la investigación tecnológica aplicada en la Ingeniería en Ciencias de la Computación. Esta clasificación responde a que el propósito central del trabajo no es la formulación de teorías abstractas, sino el diseño, la implementación y la validación de un artefacto computacional operativo, concretamente un Sistema de Evaluación Adaptativa orientado a la personalización del aprendizaje mediante el uso de modelos psicométricos y técnicas de aprendizaje automático.\\

Este tipo de investigación tecnológica aplicada se halla caracterizada por la producción de conocimiento a partir de la construcción y la evaluación sistemática de soluciones software que logran dar respuesta a problemas reales, sin perder los principios de verificabilidad, reproducibilidad y rigor experimental que caracterizan a la ingeniería de software. Por tanto, el valor científico se encuentra tanto en la arquitectura del sistema como en las pruebas empíricas recogidas durante el proceso de validación. Desde esta óptica, diferentes trabajos en el campo del aprendizaje adaptativo y los sistemas de tutoría inteligente establecen que la evaluación de este tipo de sistemas debe fundamentarse en medidas cuantitativas objetivas (precisión diagnóstica, eficiencia algorítmica, calidad predictiva, entre otras) y no en aproximaciones meramente descriptivas \cite{ref7,ref8,ref10}. Esta línea de trabajo refuerza la idoneidad del tipo de investigación escogida.\\

\subsection{Diseño experimental basado en simulación}
\vspace{0.5cm}

En relación con el diseño de la investigación, se optó por un diseño experimental ya que la investigación supone la manipulación controlada de variables independientes y la observación sistemática de sus efectos sobre variables dependientes relacionadas con el rendimiento del sistema. Las variables manipuladas incluyen el nivel de habilidad previo del estudiante, la consistencia en las respuestas, la dificultad de los ítems y la concurrencia de usuarios, mientras que las variables observadas son la convergencia de la estimación de habilidad, el error de medición, la calidad predictiva del modelo y el rendimiento computacional del sistema. El diseño experimental se implementó a través de simulación computacional, una técnica ampliamente empleada en investigaciones vinculadas con la ingeniería de software, los sistemas autoadaptativos y el aprendizaje adaptativo, particularmente en contextos donde la experimentación directa con usuarios reales se encuentra limitada por consideraciones éticas, logísticas o temporales [3], [6], [10].\\

Con este fin, se desarrolló un simulador de estudiantes virtuales capaz de generar interacciones estocásticas con el sistema de evaluación adaptativa, siguiendo un enfoque de tipo Monte Carlo, donde cada estudiante virtual fue modelado a partir de parámetros psicométricos previamente definidos, tales como la habilidad latente inicial ($\theta$), la probabilidad de dominio asociada a cada habilidad y la consistencia en las respuestas, permitiendo analizar el comportamiento del sistema frente a una amplia diversidad de perfiles de aprendizaje.\\ 

El diseño experimental por simulación permitió llevar a cabo experimentos de tipo transversal y longitudinal, así como evaluar el sistema en situaciones extremas poco reproducibles en ambientes educativos reales, como patrones de respuestas erráticas, escasez de ítems calibrados disponibles o ejecución simultánea de un elevado número de sesiones de evaluación concurrentes, contribuyendo al análisis de la robustez, tolerancia a fallos y escalabilidad del motor adaptativo previo a su implementación en contextos académicos reales. \\

La Tabla~\ref{tab:clasificacion-metodologica} presenta un resumen de los componentes centrales del tipo y diseño de investigación adoptados junto con la justificación técnica y metodológica correspondiente.

\begin{table}[H]
\centering
\caption{Clasificación metodológica y justificación técnica del estudio}
\label{tab:clasificacion-metodologica}
\renewcommand{\arraystretch}{1.2}
\begin{spacing}{1}
\begin{tabular}{|p{4.5cm}|p{4.5cm}|p{6.0cm}|}
\hline
\textbf{Elemento metodológico} & \textbf{Clasificación adoptada} & \textbf{Justificación técnica} \\
\hline
Tipo de investigación &
Tecnológica aplicada &
Desarrollo y validación de un sistema software funcional \\
\hline
Diseño de investigación &
Experimental &
Manipulación controlada de variables y medición de efectos \\
\hline
Estrategia experimental &
Simulación computacional &
Reproducibilidad y control de escenarios complejos \\
\hline
Técnica de simulación &
Monte Carlo &
Evaluación estocástica de múltiples perfiles de estudiantes \\
\hline
Horizonte de análisis &
Transversal y longitudinal &
Evaluación inmediata y análisis temporal del aprendizaje \\
\hline
\end{tabular}
\end{spacing}
\end{table}

La Figura~\ref{fig:mape_k_componente_b} ilustra el ciclo MAPE-K implementado en el sistema de evaluación adaptativa, evidenciando la correspondencia entre las fases de monitoreo, análisis, planificación y ejecución y los procesos internos del motor adaptativo desarrollado.

\begin{figure}[H]
\centering
\includegraphics[width=0.70\textwidth]{figures/mape-k.png}
\caption{Ciclo MAPE-K implementado en el Motor Adaptativo}
\label{fig:mape_k_componente_b}
\end{figure}

\section{Método de investigación}

Para llevar a cabo el desarrollo y validación del Sistema de Evaluación Adaptativa se utilizó el método de investigación hipotético-deductivo, el cual resulta ser uno de los más extendidos en los ámbitos de investigación en ingeniería y ciencias de la computación cuando se desea analizar y validar la forma de comportamiento del sistema en base a una serie de supuestos teóricos formalizados. Así, resulta altamente consistente el uso de este método de investigación para el estudio de sistemas adaptativos en los que la parte de diseño algorítmico fue formulada a partir de modelos matemáticos y de modelos probabilísticos de los que se deduce la necesidad de contrastar empíricamente su validez mediante el método de experimentación controlada y reproducible.\\

El método hipotético-deductivo se define por ser un procedimiento que parte de la observación sistemática a partir de un problema, la formulación de hipótesis explicativas, la deducción de las consecuencias observables y la posterior comprobación experimental de éstas.\\

Dentro de esta investigación, dicho enfoque hizo posible estructurar el desarrollo del motor adaptativo como proceso lógico y secuencial de forma que la teoría psicométrica subyacente, las decisiones de diseño de representación algorítmica y los resultados hallados durante el curso de validación tuvieran coherencia \cite{ref4,ref7,ref10}.\\

\begin {enumerate}
\item \textbf{Fase de observación y problematización}
\vspace{0.5cm}

A través de la etapa de observación y problematización se identificaron las limitaciones recurrentes de los sistemas de evaluación tradicionales que se caracterizan por secuencias de ítems estáticos, criterios de calificación pragmáticos y escasa capacidad de adaptación a lo que realmente sabe el estudiante. La literatura especializada en aprendizaje adaptativo y sistemas de tutoría inteligente señala precisamente que estos sistemas suelen dar lugar a evaluaciones ineficaces y diagnósticos imprecisos en contextos educativos con alta heterogeneidad en los perfiles de aprendizaje \cite{ref7,ref8}.\\

Los resultados de este análisis dieron pie a la formulación de la hipótesis central de la investigación, consistente en que la integración de un modelo híbrido que combine la Teoría de Respuesta al Ítem (IRT) para obtener una estimación global de la habilidad latente del estudiante y los modelos bayesianos de rastreo de conocimiento para obtener un monitoreo más granular de las habilidades, permite conseguir una mejora notable en la eficiencia, la precisión y la equidad diagnóstica de la evaluación frente a los métodos lineales o no adaptativos.\\

Esta hipótesis se apoya en trabajos anteriores que advierten sobre la complementariedad en la combinación de los modelos psicométricos globales y de técnicas de rastreo probabilísticas del aprendizaje a nivel de habilidad \cite{ref4,ref9,ref11}.\\

\item \textbf{Fase de deducción}
\vspace{0.5cm}

En la fase de deducción, la hipótesis propuesta se tradujo en un conjunto de decisiones de diseño dirigido a orientar la implementación del motor adaptativo. Concretamente, se decidió utilizar el modelo logístico de tres parámetros (3PL) de la Teoría de Respuesta al Ítem para estimar la habilidad latente, aplicando el método de Estimación a Posteriori esperada (EAP) con el objetivo de garantizar la estabilidad numérica y disminuir los sesgos en situaciones de información escasa.\\

A su vez, se propuso un modelo bayesiano de rastreo de conocimiento con decaimiento temporal, cuyo objetivo es modelar la probabilidad de dominio de cada habilidad y la posibilidad de una pérdida progresiva de la misma en el tiempo. De estas decisiones se dedujeron una serie de consecuencias observables que podían ser evaluadas empíricamente, tales como:
\begin{itemize}
    \item La convergencia progresiva de la estimación de la habilidad del estudiante.
    \item El error estándar de medición en decremento a medida que se administraran ítems informativos.
    \item La detección temprana de las brechas de conocimiento.
    \item La adaptación dinámica de ítems y actividades propuestas.
\end{itemize}

Se dedujo que el sistema debería conseguir niveles aceptables de precisión diagnóstica con un número reducido de ítems, a la vez que un adecuado rendimiento de cómputo en condiciones de concurrencia.\\

\item \textbf{Fase de verificación experimental}
\vspace{0.5cm}

La fase de verificación experimental se llevó a cabo mediante la administración de baterías de pruebas automatizadas y experimentos controlados fundamentados en simulación computacional. En esta etapa fue posible contrastar la lógica deducida de la propia hipótesis respecto de lo que sucedía en la realidad del comportamiento de la propuesta de trabajo.\\

Los experimentos realizados incluyeron pruebas de convergencia de la habilidad estimada, evaluaciones de eficiencia del número de ítems necesarios para la calibración del estudiante, pruebas de equidad diagnóstica mediante distintos perfiles de habilidad, y experimentaciones con la calidad predictiva de las probabilidades generadas mediante el modelo, lo que requirió ajustar algunas métricas como el Brier Score.\\

El método hipotético-deductivo permitió extender la validación del sistema más allá de su comportamiento algorítmico, incorporando también hipótesis relacionadas con la propuesta de trabajo como servicio software. En este sentido, se formularon y experimentaron supuestos relacionados con la estabilidad del sistema bajo carga, su comportamiento ante la presencia de fallos en situaciones de alta concurrencia y el cumplimiento de umbrales aceptables de latencia y escalabilidad, aspectos críticos que deben contemplarse en sistemas educativos, especialmente en aquellos que se desarrollan como artefactos basados en web 
\cite{ref10}.\\

\end{enumerate}

\section{Levantamiento de información}
La recolección de información para validar el Sistema de Evaluación Adaptativa se apoyó en técnicas propias de la Ingeniería en Ciencias de la Computación, las cuales permitieron obtener datos objetivos, reproducibles y que provienen directamente de la ejecución del sistema. Dado el carácter algorítmico, experimental y computacional de esta investigación, no se utilizaron encuestas ni instrumentos cualitativos tradicionales. En su lugar, se utilizaron simulación computacional, generación de datos sintéticos, telemetría automática y pruebas controladas de rendimiento, métodos ampliamente reconocidos en la evaluación de sistemas adaptativos, sistemas de tutoría inteligente y software basado en inteligencia artificial \cite{ref7,ref9,ref10}.\\

Estas técnicas posibilitaron la recolección de información tanto del comportamiento pedagógico del motor adaptativo como de su funcionamiento computacional como servicio software. La información recabada caracteriza adecuadamente el funcionamiento interno del sistema, permitiendo su análisis cuantitativo bajo criterios de precisión diagnóstica, eficiencia algorítmica, estabilidad operativa y escalabilidad, aspectos considerados fundamentales en la validación de sistemas auto-adaptativos \cite{ref10}.\\

\subsection{Simulación estocástica de estudiantes virtuales}
\vspace{0.5cm}

Como técnica principal de recopilación se utilizó la simulación estocástica de estudiantes, que se implementó mediante un software específico desarrollado para este propósito: el simulador de estudiantes virtuales. Este simulador crea agentes artificiales que se parametrizan según perfiles psicométricos definidos, que incluyen el nivel de habilidad latente inicial $(\theta)$, la consistencia en las respuestas, la probabilidad de acierto al azar y la tasa de aprendizaje. Estos parámetros posibilitan modelar una amplia variedad de comportamientos de aprendizaje, siguiendo los supuestos de la Teoría de Respuesta al Ítem y los modelos de rastreo de conocimiento \cite{ref4,ref9,ref11}.\\
\vspace{0.5cm}

Con la simulación se recolectó un volumen considerable de interacciones controladas entre los estudiantes virtuales y el motor adaptativo. Esto permitió analizar cómo converge la estimación de habilidad a medida que se administran más ítems, cómo va disminuyendo el error estándar de medición, y en qué medida el diagnóstico resulta equitativo cuando se aplica a distintos perfiles de estudiantes.\\

La simulación también sirvió para reproducir de forma sistemática situaciones extremas que rara vez se encuentran en la práctica educativa cotidiana: respuestas erráticas que no siguen un patrón predecible, casos de aprendizaje acelerado donde el estudiante avanza muy rápido, o situaciones de estancamiento prolongado donde no se observa progreso significativo.\\

Este tipo de escenarios son muy difíciles de estudiar con estudiantes reales, no solo por las obvias implicaciones éticas de exponer a los estudiantes a evaluaciones poco apropiadas, sino también por la complejidad práctica de controlar todas las variables en un entorno educativo auténtico.\\

\textbf{Generación de datos sintéticos}
\vspace{0.5cm}

De forma complementaria a la simulación, se emplearon técnicas de generación de datos sintéticos con el objetivo de evaluar la robustez del sistema ante distintas condiciones operativas. Mediante el uso de perfiles psicométricos y secuencias de interacción controladas, el motor adaptativo se sometió a escenarios específicamente diseñados para poner a prueba sus mecanismos de estimación, selección adaptativa y actualización del estado del estudiante. Esta estrategia resulta particularmente eficaz en la validación de sistemas adaptativos complejos, dado que la diversidad de situaciones del mundo real es difícil de abarcar completamente en las primeras etapas de desarrollo \cite{ref3,ref10}.\\

\subsection{Sistema de registro y telemetría automática}
\vspace{0.5cm}

Para el registro de información se diseñó un sistema de telemetría automática basado en archivos de auditoría estructurados en formato JSON. Este sistema registra de forma secuencial e inmutable toda interacción que procesa el motor adaptativo: la selección del ítem, la respuesta del estudiante, la estimación de habilidad latente, la probabilidad de dominio por habilidad y la recomendación que genera el motor de inferencia.
Estos registros son la fuente primaria para analizar posteriormente el comportamiento del sistema, a la vez que aseguran la trazabilidad completa de las decisiones algorítmicas que se implementan. Los archivos de auditoría permiten obtener métricas de desempeño bastante detalladas, pero más allá de eso, hacen posible reconstruir sesiones completas de forma determinista usando mecanismos de replay \cite{ref10}.\\

\subsection{Pruebas de carga y concurrencia}
\vspace{0.5cm}

Como técnica de recolección orientada al desempeño computacional, se llevaron a cabo pruebas de carga y concurrencia. Para ello se utilizaron herramientas de simulación de usuarios concurrentes que permitieron generar peticiones simultáneas al servicio de evaluación adaptativa. Durante estas pruebas se capturaron métricas de latencia, tasa de peticiones procesadas por segundo (RPS), estabilidad del sistema y comportamiento bajo condiciones de estrés. Estas métricas resultan esenciales para valorar la viabilidad del despliegue del sistema en entornos de aprendizaje auténticos con múltiples usuarios concurrentes.\\

Como se muestra en la Figura~\ref{fig:flujo-recoleccion}, la recolección de datos se ejecuta de forma automática durante la operación del sistema adaptativo. Las interacciones generadas son procesadas por el motor adaptativo para actualizar el estado del estudiante y producir recomendaciones, mientras que los eventos y métricas de desempeño son registrados como telemetría estructurada. Este flujo permite el análisis posterior, la reconstrucción de sesiones y la evaluación objetiva del comportamiento del sistema.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/segundo.png}
    \caption{Flujo de recolección de datos en el sistema adaptativo.}
    \label{fig:flujo-recoleccion}
\end{figure}

\subsection{Síntesis de las técnicas empleadas}
\vspace{0.5cm}

La combinación de estas técnicas permitió obtener una caracterización completa del comportamiento del Sistema de Evaluación Adaptativa, tanto desde la perspectiva algorítmica como desde el funcionamiento del sistema software. La Tabla~\ref{tab:tecnicas-validacion} presenta las principales técnicas de recolección de información utilizadas en el estudio, el tipo de datos obtenidos y el objetivo metodológico correspondiente.
\begin{table}[H]
\centering
\caption{Técnicas, instrumentos y datos utilizados en la validación experimental}
\label{tab:tecnicas-validacion}
\renewcommand{\arraystretch}{1.2}
\begin{spacing}{1}
\begin{tabular}{|p{2cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|}
\hline
\textbf{Técnica} & \textbf{Instrumento} & \textbf{Datos recolectados} & \textbf{Propósito metodológico} \\
\hline
Simulación estocástica &
Simulador de estudiantes virtuales &
Respuestas simuladas, convergencia de $\theta$, error estándar &
Evaluar precisión y estabilidad del modelo \\
\hline
Datos sintéticos &
Perfiles psicométricos parametrizados &
Escenarios controlados de aprendizaje &
Probar robustez y comportamiento límite \\
\hline
Telemetría automática &
Logs de auditoría en formato JSON &
Historial de sesiones, métricas internas &
Trazabilidad y análisis detallado \\
\hline
Replay determinista &
Reconstrucción desde logs &
Secuencias completas de interacción &
Verificabilidad y replicabilidad \\
\hline
Pruebas de carga &
Simulación de usuarios concurrentes &
Latencia, RPS, estabilidad &
Evaluar escalabilidad y desempeño \\
\hline
\end{tabular}
\end{spacing}
\end{table}

El conjunto de técnicas e instrumentos de recolección empleados permitió recopilar información válida, estructurada y vinculada directamente con el comportamiento real del sistema, evitando sesgos derivados de mediciones subjetivas o indirectas. Esta práctica de recolección de datos se encuentra bien establecida en la literatura sobre evaluación de sistemas adaptativos e ingeniería de software, constituyendo un enfoque con alto grado de rigor metodológico \cite{ref7,ref10}.\\

La información recolectada mediante estos instrumentos constituye la base empírica sobre la cual se fundamenta el análisis estadístico y experimental que se desarrolla en las secciones subsiguientes del marco metodológico.

\section{Población y Muestra}
La definición de la población y la muestra en estudios de validación de sistemas adaptativos basados en simulación computacional requiere un enfoque diferenciado respecto a investigaciones empíricas con participantes humanos. En el presente estudio, la población objetivo y la muestra de validación se establecieron considerando tanto el contexto educativo al cual se orienta el sistema como las características metodológicas propias de la validación algorítmica mediante técnicas de simulación estocástica.\\

\subsection{Población objetivo}
\vspace{0.5cm}

La población objetivo del Sistema de Evaluación Adaptativa está constituida por estudiantes universitarios de nivel superior. Esta población presenta una heterogeneidad muy elevada en cuanto a conocimientos previos, ritmos de aprendizaje y estrategias de estudio, aspectos que justifican la necesidad de sistemas de evaluación personalizados y adaptativos.\\

Desde una perspectiva psicométrica, esta población puede representarse mediante un continuo de habilidad latente ($\theta$) que refleja el nivel de dominio del contenido evaluado. En el marco de la Teoría de Respuesta al Ítem (IRT), la habilidad latente en poblaciones universitarias típicamente se distribuye en un rango aproximado de θ ∈ [-3.0, +3.0] en la escala \textit{logit}, donde los valores negativos representan estudiantes con conocimientos insuficientes, los valores cercanos a cero corresponden a estudiantes de nivel medio, y los valores positivos indican un dominio avanzado o experto del tema \cite{ref4,ref5}.\\

La delimitación de esta población objetivo resulta fundamental para interpretar adecuadamente los resultados de la validación y establecer los límites de generalización del sistema desarrollado. Si bien el presente estudio se realizó mediante simulación computacional, la caracterización explícita de la población objetivo guía tanto el diseño de los perfiles de estudiantes virtuales como la futura implementación del sistema en contextos educativos reales.\\

\subsection{Muestra de validación}
\vspace{0.5cm}

La muestra de validación estuvo conformada por $N = 10$ perfiles de estudiantes virtuales, caracterizados por un conjunto de parámetros psicométricos controlados y conocidos a priori. Estos perfiles fueron diseñados para cubrir de manera sistemática el espectro de habilidad latente de la población objetivo, permitiendo evaluar el comportamiento del sistema adaptativo ante distintos niveles de conocimiento.

\begin{itemize}
    \item \textbf{Habilidad latente real ($\theta$):} 
    Distribuida uniformemente en el intervalo $[-2.0, +2.0]$, asignando valores específicos 
    $\{-2.0, -1.5, -1.0, -0.5, 0.0, +0.5, +1.0, +1.5, +2.0\}$, además de un valor aleatorio dentro del intervalo.

    \item \textbf{Probabilidad de dominio inicial (\textit{Mastery}):} 
    Modelada de forma correlacionada con la habilidad latente mediante la relación
    $p_{\text{mastery}} \approx (\theta + 2)/4$, incorporando variación estocástica gaussiana con $\sigma = 0.15$ y normalización en el intervalo $[0.1, 0.9]$.

    \item \textbf{Consistencia de respuesta:} 
    Distribuida uniformemente en el intervalo $[0.80, 0.95]$.

    \item \textbf{Tasa de aprendizaje (\textit{learning rate}):} 
    Distribuida uniformemente en el intervalo $[0.10, 0.20]$.

    \item \textbf{Factor de fatiga:} 
    Distribuido uniformemente en el intervalo $[0.01, 0.03]$ por ítem administrado.

    \item \textbf{Fundamentación teórica:} 
    La parametrización se fundamentó en modelos de la Teoría de Respuesta al Ítem y en estudios empíricos sobre patrones de respuesta en evaluaciones adaptativas \cite{ref4,ref9,ref11}.
\end{itemize}

\subsection{Banco de ítems}
\vspace{0.5cm}

El banco de ítems utilizado para la validación del sistema estuvo conformado por 200 ítems de opción múltiple orientados a la evaluación de conocimientos sobre derivadas. Cada ítem fue caracterizado mediante el modelo logístico de tres parámetros (3PL) de la Teoría de Respuesta al Ítem, considerando los siguientes parámetros psicométricos:

\begin{enumerate}[label=(\alph*)]
    \item \textbf{Parámetro de discriminación ($a$):} 
    distribuido según una distribución log-normal con media $\mu = 0.3$ y desviación estándar $\sigma = 0.4$, con valores truncados en el intervalo $[0.5, 2.5]$.

    \item \textbf{Parámetro de dificultad ($b$):} 
    distribuido uniformemente en el intervalo $[-3.0, +3.0]$, con una distribución aproximada de $33\%$ de ítems fáciles, $33\%$ de dificultad media y $33\%$ de ítems difíciles.

    \item \textbf{Parámetro de adivinanza ($c$):} 
    distribuido uniformemente en el intervalo $[0.0, 0.25]$.
\end{enumerate}

Los ítems se distribuyeron equitativamente entre dos habilidades del dominio de derivadas: regla de la potencia (100 ítems) y regla de la cadena (100 ítems), asegurando una cobertura balanceada para la evaluación adaptativa.\\

Dado el carácter de validación algorítmica del estudio, los parámetros IRT fueron generados de forma sintética mediante procedimientos estocásticos controlados, práctica común en fases tempranas de desarrollo de sistemas adaptativos. Esta decisión se reconoce como una limitación que será abordada en etapas posteriores mediante calibración con datos reales  \cite{ref3,ref10}.

\subsection{Justificación del tamaño muestral}
\vspace{0.5cm}

La determinación del tamaño muestral en estudios de validación mediante simulación computacional responde a criterios distintos de aquellos utilizados en investigaciones con participantes humanos. En lugar de fundamentarse en cálculos de potencia estadística para detectar diferencias entre grupos, el tamaño muestral en simulaciones se orienta a garantizar la cobertura representativa del espacio de parámetros y la estabilidad de las estimaciones obtenidas mediante métodos Monte Carlo \cite{ref3,ref9}.\\

La literatura especializada en evaluación de sistemas de testing adaptativo computarizado (CAT) y algoritmos de selección de ítems recomienda un tamaño muestral mínimo de N ≥ 10 perfiles de estudiantes virtuales para validaciones iniciales de tipo algorítmico, siempre que estos perfiles cubran de manera sistemática el rango de habilidad de interés y presenten heterogeneidad en sus características de respuesta \cite{ref9,ref11}.\\

Este tamaño permite evaluar la estabilidad del algoritmo, la convergencia de las estimaciones y la ausencia de sesgos sistemáticos en distintos niveles de habilidad.\\

En el presente estudio, el tamaño muestral de N = 10 fue considerado suficiente para los siguientes propósitos metodológicos:

\begin{itemize}
    \item \textbf{Evaluación de convergencia:} 
    analizar si el algoritmo de estimación de habilidad mediante EAP converge de forma estable hacia el valor verdadero de $\theta$ en distintos niveles del continuo de habilidad.

    \item \textbf{Análisis de equidad diagnóstica:} 
    verificar que el sistema no presente sesgos sistemáticos en la precisión de las estimaciones entre estudiantes con bajo, medio y alto rendimiento.

    \item \textbf{Evaluación de eficiencia:} 
    determinar el número promedio de ítems requeridos para alcanzar niveles aceptables de precisión diagnóstica, definidos como $SE(\theta) \leq 0.4$, en distintos perfiles de estudiantes.

    \item \textbf{Detección de fallos algorítmicos:} 
    identificar posibles errores lógicos, condiciones de borde no controladas o comportamientos anómalos del sistema bajo escenarios diversos.
\end{itemize}

Cada perfil de estudiante virtual fue sometido a sesiones de evaluación de hasta 20 ítems, lo que permitió generar aproximadamente 200 interacciones ítem–estudiante registradas. Este volumen de datos resultó suficiente para el cálculo de métricas agregadas con niveles de error estándar aceptables, así como para la realización de análisis de sensibilidad bajo distintas condiciones de operación del sistema.\\

Si bien el tamaño muestral de $N = 10$ es adecuado para la validación técnica y algorítmica del motor adaptativo, no permite realizar inferencias estadísticas generalizables a la población de estudiantes reales. En este sentido, la presente fase corresponde a una validación de carácter técnico, orientada a verificar el correcto funcionamiento del sistema antes de su despliegue en contextos educativos reales. La utilización de simulación computacional como estrategia de validación inicial ofrece ventajas metodológicas relevantes, como el control riguroso de las variables experimentales, la reproducibilidad de los escenarios evaluados y la posibilidad de analizar comportamientos extremos, fortaleciendo así la validez interna del estudio \cite{ref3,ref9,ref10}.\\

\section{Variables de la Investigación}

La identificación, operacionalización y clasificación de las variables de estudio constituyen un componente esencial del diseño experimental de la presente investigación. En este estudio, las variables fueron definidas de acuerdo con los principios de la Teoría de Respuesta al Ítem y los modelos bayesianos de rastreo de conocimiento, garantizando su medibilidad, reproducibilidad y coherencia con el marco teórico adoptado \cite{ref4,ref5}.\\

Las variables se clasificaron en tres categorías principales: variables independientes, correspondientes a los parámetros controlados o manipulados durante la simulación; variables dependientes, que representan las salidas generadas por el Sistema de Evaluación Adaptativa; y variables de control, que permanecieron constantes durante los experimentos con el fin de aislar los efectos de las variables independientes sobre las dependientes. Esta clasificación facilita el análisis causal y la correcta interpretación de los resultados obtenidos \cite{ref10}.\\

\begin{enumerate}
\item \textbf{Variables independientes}
\vspace{0.5cm}

Las variables independientes corresponden a los parámetros psicométricos y comportamentales de los estudiantes virtuales, así como a las características de los ítems administrados, los cuales fueron manipulados de forma controlada durante la simulación para evaluar su impacto sobre el desempeño del motor adaptativo. Entre estas variables se incluyen la habilidad latente real del estudiante ($\theta$), los parámetros psicométricos de los ítems definidos por el modelo IRT de tres parámetros (discriminación $a$, dificultad $b$ y adivinanza $c$), la consistencia de respuesta, la tasa de aprendizaje y el factor de fatiga, todos ellos operacionalizados mediante rangos numéricos controlados y coherentes con la literatura especializada \cite{ref4,ref9,ref11}.\\

\item \textbf{Variables dependientes}
\vspace{0.5cm}

Las variables dependientes corresponden a las salidas generadas por el Sistema de Evaluación Adaptativa durante el procesamiento de las respuestas de los estudiantes virtuales y constituyen los principales objetos de medición del estudio. Estas variables permiten evaluar la precisión, eficiencia y calidad del sistema, e incluyen la habilidad estimada ($\hat{\theta}$), el error estándar de la estimación ($SE(\hat{\theta})$), el error de estimación absoluto ($|\theta - \hat{\theta}|$), la probabilidad de dominio por habilidad ($p_{\text{mastery}}$), el \textit{Brier Score}, la latencia de respuesta del sistema y el número de ítems administrados hasta alcanzar la convergencia diagnóstica.

\item \textbf{Variables de control}
\vspace{0.5cm}

Las variables de control corresponden a parámetros del sistema que permanecieron constantes durante todas las ejecuciones experimentales, con el propósito de garantizar la validez interna del diseño y aislar los efectos de las variables independientes. Estas variables incluyen los parámetros del modelo BKT por habilidad, la configuración del algoritmo de estimación EAP, los umbrales de decisión definidos para el dominio y la precisión diagnóstica, los parámetros de \textit{decay} temporal y los límites operativos del sistema, tales como el número máximo de ítems por sesión y las restricciones de repetición de ítems \cite{ref10}.

\end{enumerate}

\textbf{Síntesis de variables}
\vspace{0.5cm}

La Tabla~\ref{tab:variables-estudio} presenta una síntesis de las variables del estudio, clasificadas según su rol en el diseño experimental, e incluye su descripción, unidad de medida y rangos de valores observados o asignados.\\

\begin{table}[H]
\centering
\small
\caption{Definición y clasificación de variables del estudio}
\label{tab:variables-estudio}
\renewcommand{\arraystretch}{1.2}
\begin{spacing}{1}
\begin{tabular}{|p{2.2cm}|p{3.0cm}|p{5.0cm}|p{2.2cm}|p{2.2cm}|}
\hline
\textbf{Tipo} & \textbf{Variable} & \textbf{Descripción} & \textbf{Unidad} & \textbf{Rango} \\
\hline
Independiente &
$\theta$ (habilidad real) &
Nivel verdadero de conocimiento &
Escala logit &
$[-2.0, +2.0]$ \\
\hline
Independiente &
$a$ (discriminación) &
Capacidad discriminativa del ítem &
Adimensional &
$[0.5, 2.5]$ \\
\hline
Independiente &
$b$ (dificultad) &
Nivel de dificultad del ítem &
Escala logit &
$[-3.0, +3.0]$ \\
\hline
Independiente &
$c$ (adivinanza) &
Probabilidad de acierto por azar &
Probabilidad &
$[0.0, 0.25]$ \\
\hline
Independiente &
Consistencia &
Coherencia en las respuestas &
Probabilidad &
$[0.80, 0.95]$ \\
\hline
Independiente &
\textit{Learning rate} &
Tasa de aprendizaje incremental &
Por ítem &
$[0.10, 0.20]$ \\
\hline
Independiente &
Factor de fatiga &
Incremento de tiempo por fatiga &
Por ítem &
$[0.01, 0.03]$ \\
\hline
Dependiente &
$\hat{\theta}$ (habilidad estimada) &
Estimación EAP de la habilidad &
Escala logit &
$[-4.0, +4.0]$ \\
\hline
Dependiente &
$SE(\hat{\theta})$ &
Error estándar de la estimación &
Escala logit &
$[0.2, 1.0]$ \\
\hline
Dependiente &
$|\theta - \hat{\theta}|$ &
Error de estimación absoluto &
Escala logit &
$[0.0, 2.0]$ \\
\hline
Dependiente &
$p_{\text{mastery}}$ &
Probabilidad de dominio por habilidad &
Probabilidad &
$[0.0, 1.0]$ \\
\hline
Dependiente &
Brier Score &
Calibración predictiva &
Error cuadrático &
$[0.0, 1.0]$ \\
\hline
Dependiente &
Latencia &
Tiempo de respuesta del sistema &
Milisegundos &
$[50, 500]$ \\
\hline
Dependiente &
$N$ ítems de convergencia &
Ítems requeridos hasta $SE(\hat{\theta}) \leq 0.4$ &
Cantidad &
$[5, 20]$ \\
\hline
Control &
Parámetros BKT &
$p_{L0}, p_T, p_G, p_S, p_F$ &
Probabilidades &
Fijos por \textit{skill} \\
\hline
Control &
Configuración EAP &
Grid, prior $N(0,1)$ &
-- &
Fijos \\
\hline
Control &
Umbrales &
$\tau$, $SE$ objetivo, límites &
-- &
Fijos \\
\hline
\end{tabular}
\end{spacing}
\end{table}

\section{Marco metodológico de desarrollo}
El desarrollo del Sistema de Evaluación Adaptativa se llevó a cabo siguiendo una metodología propia de la Ingeniería del Software, lo que hizo posible una construcción sistemática, controlada y alineada con buenas prácticas de desarrollo. Bajo esta lógica, se adoptó la metodología ágil SCRUM como marco de gestión del proceso de desarrollo, complementando el método de investigación hipotético-deductivo descrito anteriormente.\\

Vale la pena aclarar aquí un punto que puede generar confusión: SCRUM no fue empleado como método de investigación científica en sí mismo, sino como un mecanismo práctico para organizar, planificar y dar seguimiento al trabajo técnico que implicaba construir el sistema.\\

\subsection{Justificación de la elección metodológica}
\vspace{0.5cm}

La decisión de trabajar con SCRUM tiene que ver directamente con la naturaleza del sistema que se estaba desarrollando. El motor adaptativo integra varios módulos que dependen unos de otros: modelos psicométricos, lógica de selección adaptativa de ítems, persistencia del estado del estudiante, instrumentación de métricas y mecanismos de validación. Estos componentes necesitaban ciclos cortos de desarrollo, prueba y ajuste para poder integrarse correctamente. Es como armar un mecanismo complejo donde cada pieza debe encajar con precisión, pero solo se puede verificar que funciona una vez que las partes están conectadas. SCRUM dio la flexibilidad necesaria para ir incorporando funcionalidades poco a poco y ver en tiempo real cómo afectaban al comportamiento global del sistema \cite{ref14,ref15}. \\

Existe otra razón de peso para elegir metodologías ágiles cuando se trabaja con sistemas basados en inteligencia artificial y aprendizaje automático. La incertidumbre es alta: no siempre se puede anticipar cómo va a comportarse un modelo psicométrico bajo condiciones reales, o qué impacto tendrá modificar ciertos parámetros de convergencia. Los requisitos técnicos también suelen evolucionar conforme se van realizando experimentos y se obtienen resultados inesperados. SCRUM permite ajustar el plan de desarrollo según lo que va mostrando la evidencia empírica en cada iteración, lo que reduce bastante el riesgo de tomar decisiones de diseño que después resulten desconectadas de los resultados experimentales \cite{ref10}.\\

\subsection{Estructura y organización de los sprints}
\vspace{0.5cm}

El marco SCRUM que se aplicó en este proyecto se organizó mediante sprints de una semana cada uno, con objetivos técnicos específicos y entregables que se podían verificar.\\

Elegir una semana no fue casualidad: es un período que permite ver resultados tangibles sin tener que esperar demasiado, pero a la vez da tiempo suficiente para implementar funcionalidades que cumplan con estándares mínimos de calidad.\\

El desarrollo de cada iteración obedecía a una progresión definida de las siguientes tareas:
\begin{itemize}
    \item \textbf{Planificación del sprint:} definir los objetivos técnicos y seleccionar las tareas del backlog que se abordarían.
    \item \textbf{Desarrollo iterativo:} implementar las funcionalidades que se habían priorizado.
    \item \textbf{Pruebas unitarias:} verificar que cada componente individual funcionara como debía.
    \item \textbf{Pruebas de integración:} comprobar que los módulos interactuaran correctamente entre sí.
    \item \textbf{Validación funcional:} confirmar que se cumplieran los requisitos técnicos y algorítmicos establecidos.
\end{itemize}

Este proceso garantizaba que cada incremento tuviera un nivel mínimo de calidad antes de integrarse al sistema base. La idea era evitar acumular deuda técnica que después pudiera comprometer la estabilidad del motor adaptativo cuando el sistema creciera en complejidad.\\

\subsection{Adaptación al contexto académico}
\vspace{0.5cm}

Hay que señalar algo importante: la forma en que se usó SCRUM aquí no es exactamente igual a como se usa en un proyecto comercial típico. En una empresa, cada sprint busca entregar valor tangible al cliente o al usuario final. En este caso, cada sprint se centraba más bien en validar técnica y algorítmicamente el sistema, siguiendo las hipótesis de investigación que se habían planteado. Los entregables no se medían tanto por funcionalidades listas para producción, sino por componentes validados empíricamente que confirmaban o cuestionaban aspectos específicos del diseño propuesto.\\

Esta adaptación hizo que el proceso de desarrollo estuviera muy conectado con el proceso investigativo, sin sacrificar el rigor metodológico ni perder la trazabilidad de las decisiones técnicas. Cada decisión de diseño, cada ajuste que se hacía en los algoritmos, cada refactorización importante quedaba documentada y justificada según los resultados experimentales que se iban obteniendo. El desarrollo técnico no era un fin en sí mismo, sino más bien una forma de responder las preguntas de investigación que se habían formulado al inicio.\\

 \subsection{Planificación progresiva y mejora continua}
 \vspace{0.5cm}

La Tabla~\ref{tab:sprints} detalla la distribución temporal de los sprints que se desarrollaron a lo largo del ciclo de construcción del Sistema de Evaluación Adaptativa, destacando los objetivos técnicos que se alcanzaron en cada etapa del proyecto.

\begin{table}[H]
\centering
\caption{Planificación incremental de sprints para el desarrollo del motor adaptativo}
\label{tab:sprints}
\renewcommand{\arraystretch}{1.2}
\begin{spacing}{1}
\begin{tabular}{|p{3.0cm}|p{5.5cm}|p{6.5cm}|}
\hline
\textbf{Sprint} & \textbf{Objetivo principal} & \textbf{Resultados obtenidos} \\
\hline
Sprint 0--1 &
Investigación y diseño inicial &
Selección de frameworks, definición de arquitectura y contratos JSON \\
\hline
Sprint 2--3 &
Diseño del modelo adaptativo &
Implementación del modelo híbrido IRT+BKT \\
\hline
Sprint 4--5 &
Implementación algorítmica &
Estimación EAP, selección adaptativa de ítems y \textit{decay} temporal \\
\hline
Sprint 6 &
Validación experimental &
Simulación, pruebas automatizadas y pruebas de carga \\
\hline
Sprint 7 &
Refactorización y documentación &
Optimización del código, documentación técnica y cierre del desarrollo \\
\hline
\end{tabular}
\end{spacing}
\end{table}

La Figura~\ref{fig:arquitectura} presenta la arquitectura general del Sistema de Evaluación Adaptativa, resultado del proceso de diseño e implementación desarrollado durante los sprints iniciales del proyecto. Esta arquitectura refleja la organización modular del motor adaptativo y los componentes principales que fueron validados experimentalmente. El código fuente completo y la implementación reproducible del sistema se encuentran disponibles en el repositorio del proyecto, referenciado en el Anexo~\ref{anexo:repositorio}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/arquitectura.png}
    \caption{Flujo de recolección de datos en el sistema adaptativo.}
    \label{fig:arquitectura}
\end{figure}

\subsection{Resultados de la aplicación de SCRUM}
\vspace{0.5cm}

Usar SCRUM contribuyó bastante a manejar la complejidad que implicaba desarrollar este sistema. Entre los beneficios más evidentes están:

\begin{itemize}
    \item \textbf{Identificación temprana de errores:} detectar problemas en etapas tempranas mediante la validación continua, cuando corregirlos resulta significativamente menos costoso en términos de tiempo y esfuerzo.
    \item \textbf{Validación incremental:} probar a fondo cada funcionalidad antes de pasar a la siguiente, asegurando tener una base sólida y estable para seguir construyendo.
    \item \textbf{Mejoras basadas en datos:} fundamentar los cambios del sistema en evidencia experimental concreta, no en especulaciones sobre cómo debería comportarse.
    \item \textbf{Trazabilidad completa:} mantener una alineación constante entre los objetivos de investigación, las decisiones de diseño y los resultados que se iban obteniendo mediante la estructura iterativa.
\end{itemize}

Esta trazabilidad tiene un valor especial en un contexto académico, donde poder reproducir el trabajo y justificar rigurosamente las decisiones técnicas son aspectos centrales del proceso investigativo. Cada sprint generó documentación detallada que permitiría a otros investigadores entender no solo qué se implementó, sino por qué se tomaron determinadas decisiones de diseño.\\

El marco metodológico adoptado garantizó que el Sistema de Evaluación Adaptativa se desarrollara siguiendo principios de calidad de software, mantenibilidad y extensibilidad. Estos aspectos son fundamentales tanto para la futura integración del sistema con otros componentes del ecosistema de aprendizaje como para su eventual implementación en entornos educativos reales. La combinación del método hipotético-deductivo para la investigación y SCRUM para el desarrollo técnico resultó efectiva, permitiendo mantener el rigor científico mientras se construía un artefacto computacional que realmente funciona \cite{ref14, ref15}.\\

\section{Actividades y productos del proyecto}

Las actividades ejecutadas para cada objetivo generaron productos y evidencias técnicas concretas en forma de artefactos de diseño, módulos de software funcionales, registros de simulación y métricas de rendimiento, lo que permitió comprobar de manera objetiva el grado de cumplimiento de los objetivos específicos \cite{ref7,ref10}.\\

La Tabla~\ref{tab:objetivos-actividades} presenta la correspondencia entre los objetivos específicos del proyecto, las principales actividades desarrolladas y los productos generados.

\begin{table}[H]
\centering
\caption{Correspondencia entre objetivos específicos, actividades y productos generados}
\label{tab:objetivos-actividades}
\renewcommand{\arraystretch}{1.2}
\begin{spacing}{1}
\begin{tabular}{|p{4.5cm}|p{6.5cm}|p{3.5cm}|}
\hline
\textbf{Objetivo específico} & \textbf{Actividades desarrolladas} & \textbf{Productos / evidencias} \\
\hline
Analizar herramientas de IA aplicables a la evaluación adaptativa &
Revisión del estado del arte en aprendizaje adaptativo, ITS e IRT. Análisis comparativo de modelos psicométricos y de rastreo de conocimiento. &
Selección fundamentada del modelo híbrido IRT (3PL) + BKT \\
\hline
Diseñar un sistema de evaluación progresiva y personalizada &
Diseño de la arquitectura del motor adaptativo y definición de contratos de comunicación y reglas de selección de ítems. &
Arquitectura del sistema adaptativo y esquemas JSON \\
\hline
Implementar modelos de análisis y seguimiento del aprendizaje &
Implementación del modelo IRT con EAP y del modelo BKT con decaimiento temporal. Integración de métricas. &
Motor adaptativo funcional y módulos de cálculo \\
\hline
Validar el sistema mediante pruebas funcionales y experimentales &
Simulación de estudiantes virtuales, pruebas de convergencia, eficiencia, equidad y pruebas de carga. &
Resultados de simulación, reportes de pruebas y métricas de rendimiento \\
\hline
\end{tabular}
\end{spacing}
\end{table}

\section{Técnicas de Análisis de la Información}

El análisis de la información obtenida durante la validación del Sistema de Evaluación Adaptativa se centró en evaluar tanto el comportamiento algorítmico del motor adaptativo como el rendimiento computacional del sistema, utilizando métricas objetivas, numéricas
y reproducibles \cite{ref7,ref10}.\\

\begin{enumerate}
\item \textbf{Análisis del rendimiento algorítmico}
\vspace{0.5cm}

Desde el punto de vista del rendimiento algorítmico, el análisis realizado se centró en evaluar la precisión del modelo híbrido implementado considerando métricas de error ampliamente utilizadas en psicometría computacional. Se utilizaron el error cuadrático medio (RMSE) y el error absoluto medio (MAE) para evaluar la precisión del sistema en la estimación de habilidades latentes, métricas ampliamente utilizadas en psicometría computacional \cite{ref4,ref5,ref11}.\\

El análisis incluyó la evolución del error estándar de medición para observar la convergencia del modelo y la reducción de la incertidumbre diagnóstica conforme se administran ítems informativos \cite{ref4,ref9}.\\

La calidad predictiva del sistema fue analizada mediante el Brier Score, que permitió evaluar la calibración de las probabilidades de respuesta correcta predichas por el sistema frente a los resultados observados en la simulación \cite{ref11,ref12}.\\

Junto con el Brier Score, también se implementó un análisis longitudinal del aprendizaje simulado dirigido a comprobar la capacidad del sistema para detectar los cambios en el dominio de las habilidades a lo largo del tiempo contemplando medidas de adquisición progresiva de los conocimientos y fenómenos del tipo de decaimiento o pérdida del dominio del mismo tipo, esto permitiría comprobar la sensibilidad del modelo frente a los cambios temporales en el rendimiento del estudiante. Este tipo de análisis es muy relevante en sistemas de evaluación adaptativa que intentan proporcionar retroalimentación continua y personalizada \cite{ref9,ref11}.\\

\item \textbf{Análisis del rendimiento computacional}
\vspace{0.5cm}

El rendimiento computacional se evaluó mediante métricas de latencia de respuesta (en milisegundos), percentiles de tiempo de respuesta (P50 y P95) y tasa de peticiones por segundo procesadas (RPS), obtenidas durante las pruebas de carga y concurrencia \cite{ref10}.\\

Estas métricas permitieron evaluar la capacidad del sistema para operar como un servicio software en condiciones de uso realistas, garantizando tiempos de respuesta adecuados para aplicaciones interactivas y estabilidad bajo carga concurrente.\\


\end {enumerate}
\subsection{Herramientas y reproducibilidad del análisis}
\vspace{0.5cm}

Las técnicas de análisis se implementaron utilizando librerías estándar del ecosistema
Python, garantizando la transparencia y reproducibilidad de los resultados \cite{ref10}.\\

La Tabla \ref{tab:dimensiones-metricas}, resume las dimensiones y métricas utilizadas para evaluar el desempeño del motor adaptativo.

\begin{table}[H]
\centering
\caption{Dimensiones y métricas utilizadas para la evaluación del desempeño del motor adaptativo}
\label{tab:dimensiones-metricas}
\renewcommand{\arraystretch}{1.2}
\begin{spacing}{1}
\begin{tabular}{|p{2.2cm}|p{2.5cm}|p{4cm}|p{5.0cm}|}
\hline
\textbf{Dimensión analizada} & \textbf{Métrica} & \textbf{Descripción} & \textbf{Propósito del análisis} \\
\hline
Precisión diagnóstica &
RMSE / MAE &
Error entre habilidad real y estimada &
Evaluar exactitud del modelo \\
\hline
Convergencia &
Error estándar de medición &
Nivel de incertidumbre en la estimación de $\theta$ &
Analizar eficiencia adaptativa \\
\hline
Calidad predictiva &
Brier Score &
Calibración de probabilidades predichas &
Validar confiabilidad del modelo \\
\hline
Rendimiento &
Latencia (ms) &
Tiempo de respuesta del sistema &
Evaluar experiencia del usuario \\
\hline
Escalabilidad &
RPS, P50/P95 &
Capacidad bajo concurrencia &
Analizar viabilidad operativa \\
\hline
\end{tabular}
\end{spacing}
\end{table}


\section{Criterios de Validación y Aceptación}
\begin{enumerate}
\item \textbf{Criterios de Equidad en la Estimación Diagnóstica}
\vspace{0.5cm}

La equidad en la estimación diagnóstica tiene como objetivo mantener un nivel de precisión similar entre los estudiantes de diferentes niveles de la misma habilidad, ya que se busca evitar que el sistema beneficie o penalice a unos estudiantes más que a otros de manera sistemática, conforme a los principios de equidad en la medición educativa reportados en la literatura psicométrica \cite{ref4,ref5}.\\

Para evaluar este criterio, se llevó a cabo el análisis de la consistencia del error de estimación, considerando el valor del RMSE para una variable en tres grupos definidos según la habilidad real del estudiante:
\begin{itemize}
    \item Grupo bajo: $\theta \in [-2.0, -0.67)$
    \item Grupo medio: $\theta \in [-0.67, +0.67]$
    \item Grupo alto: $\theta \in (+0.67, +2.0]$
\end{itemize}

El criterio de aceptación determinado fue un coeficiente de variación del RMSE inferior al $40\%$, definido como la razón entre la desviación estándar y la media del RMSE. Asimismo, se estableció como criterio complementario que el valor máximo registrado de RMSE no superara $0.70$, con el fin de evitar que algún grupo presente un error de estimación excesivo. Esta consideración se encuentra alineada con los criterios de equidad utilizados en sistemas de evaluación adaptativa y medición educativa \cite{ref4,ref10}.\\

\item \textbf{Criterios de Calidad Predictiva}
\vspace{0.5cm}

El criterio de calidad predictiva mide qué tan bien las probabilidades que calcula el modelo acerca de si un estudiante responderá correctamente o no coinciden con los resultados observados durante la simulación, aspecto fundamental en sistemas de evaluación adaptativa basados en modelos probabilísticos \cite{ref9,ref11}.\\

Con este propósito, se realizó el cálculo del \textit{Brier Score}, definido como el error cuadrático medio entre las probabilidades predichas y los valores binarios de acierto o error observados. Como criterio de aceptación se consideraron valores inferiores a $0.30$, dado que dichos valores indican una capacidad predictiva superior a la aleatoriedad y una calibración adecuada del modelo, consistente con lo reportado en la literatura para modelos IRT y BKT en contextos educativos estructurados \cite{ref11,ref12}.\\

\item \textbf{Criterios de Rendimiento Computacional}
\vspace{0.5cm}

El rendimiento computacional se refiere a la capacidad del sistema para operar como un servicio software real, respetando tiempos de respuesta adecuados en situaciones de carga similares a las que se podrían presentar en un contexto educativo real, tal como se exige en sistemas auto-adaptativos desplegados en entornos productivos \cite{ref10}.\\

En este sentido, se definió como criterio de aceptación que la latencia de respuesta P95 fuera inferior a $500$ ms, garantizando tiempos de respuesta apropiados para aplicaciones interactivas. Adicionalmente, se estableció como criterio de estabilidad bajo concurrencia que la tasa de error fuera inferior al $1\%$ bajo una carga de $50$ usuarios concurrentes, asegurando así un nivel de disponibilidad compatible con servicios educativos donde la continuidad del proceso de evaluación resulta crítica \cite{ref10}.\\

\end{enumerate}
\vspace{0.5cm}
\subsection{Síntesis e interpretación de los criterios}
\vspace{0.5cm}

Los criterios de validación fueron definidos previamente a la ejecución de los experimentos, siguiendo el principio metodológico de pre-especificación de hipótesis, ampliamente recomendado en estudios experimentales de ingeniería de software y sistemas auto-adaptativos \cite{ref10}.\\

La verificación de los criterios de aceptación se realizó de forma sistemática durante el análisis de los resultados, clasificando el comportamiento del sistema como criterio cumplido, parcialmente cumplido o no cumplido. El cumplimiento de al menos seis de los siete criterios establecidos se consideró evidencia suficiente para validar técnicamente el Sistema de Evaluación Adaptativa.\\

En este sentido, los criterios definidos proporcionan el marco de referencia necesario para interpretar de manera objetiva los resultados experimentales que se presentan a continuación, permitiendo evaluar el desempeño del sistema en términos de precisión diagnóstica, equidad, calidad predictiva y viabilidad operativa. Cabe señalar que estos criterios corresponden a una fase de validación técnica y algorítmica basada en simulación computacional, mientras que la evaluación en contextos educativos reales requerirá criterios adicionales relacionados con la usabilidad, la aceptación pedagógica y el impacto en el aprendizaje, tal como se discute en estudios previos sobre sistemas de tutoría inteligente \cite{ref7,ref8}.


